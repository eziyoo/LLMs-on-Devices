# ğŸ§  LlamaAndroid: On-Device LLM Inference for Android

Welcome to the official repository for my thesis project: **on-device large language model inference using Llama.cpp on Android**. This project explores the feasibility, performance, and usability of running quantized LLMs natively on Android devices â€” with no server-side dependency.

---

## ğŸ“± About the Project

This Android app demonstrates how to:
- Run LLMs directly on-device using [llama.cpp](https://github.com/ggerganov/llama.cpp)
- Download GGUF models dynamically from Hugging Face
- Interact with the model via a simple chat interface
- Benchmark performance under different configurations

This setup is designed to help evaluate the **efficiency and limitations of local inference** on mobile hardware, especially for research and offline use cases.

---

## ğŸ“‚ Project Structure

```
thesis-repo/
â”œâ”€â”€ app/                  # Android app code (Jetpack Compose UI)
â”œâ”€â”€ native/llama.cpp/     # Git submodule: forked llama.cpp backend
â”œâ”€â”€ figures/              # Thesis figures and visual assets
â”œâ”€â”€ build.gradle          # Root Gradle config
â””â”€â”€ README.md             # You're here!
```

---

## ğŸ”§ Setup Instructions

### 1. Clone with Submodule

```bash
git clone --recurse-submodules https://github.com/your-username/thesis-repo.git
cd thesis-repo
```

### 2. Open in Android Studio

- Make sure youâ€™re using the **NDK** and **CMake**
- Sync Gradle and let Android Studio build native sources

### 3. Run on Device

- Connect your Android device or use an emulator with sufficient RAM
- Click **Run** in Android Studio
- Use the dropdown to select and download a GGUF model
- Start chatting!

---

## ğŸ§ª Models Included

The app includes downloadable links to quantized versions of:

- âœ… Phi-2 7B (Q4_0)
- âœ… TinyLlama 1.1B (f16)
- âœ… Phi-2 DPO (Q3_K_M)
- âœ… Add your own model easily via the `MainActivity.kt` model list

---

## ğŸ“Š Benchmarking

Use the **Bench** button in the app to run performance tests. Results include:
- Token throughput
- Warm-up time
- Memory usage

This feature helps evaluate real-world performance across model sizes and device types.

---

## ğŸ“š Thesis Focus

My thesis explores:
- Feasibility of local LLM inference on consumer-grade Android hardware
- Trade-offs in model size, quantization, latency, and UX
- Application design challenges with native + Compose + JNI

---

## ğŸ”— Dependencies

- [llama.cpp (fork)](https://github.com/ehsaani/llama.cpp)
- Android Jetpack Compose
- CMake + NDK (for JNI integration)

---

## ğŸ™‹â€â™‚ï¸ Author

**Ehsaan I.**  
Thesis candidate @ [Your University]  
ğŸ“§ [your-email@example.com]  
ğŸ”— [your-linkedin-or-website.com]

---

## ğŸ“ License

This project is for academic and research purposes. See individual licenses for dependencies like `llama.cpp`.
